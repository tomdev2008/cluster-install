#!/bin/bash
# ----------------------------------------
# base information
# ----------------------------------------
servers=g9t3406.houston.hpicorp.net,g9t3412.houston.hpicorp.net,g9t3279.houston.hpicorp.net,g9t3429.houston.hpicorp.net,g9t3428.houston.hpicorp.net,g9t3280.houston.hpicorp.net,g9t3411.houston.hpicorp.net,g9t3410.houston.hpicorp.net,g9t3430.houston.hpicorp.net,g9t3421.houston.hpicorp.net
if [ ${servers_test:-""} != "" ]; then servers=$servers_test; fi
declare -a servers_cwp=\(\"${servers//,/\" \"}\"\)

master=c1t14285.itcs.hpicorp.net
user=xu6
password=CDEszaq821202
aapath=/home/xu6/grid
aapath1=/home/xu6/grid

# ----------------------------------------
# cluster-environment and install
# ----------------------------------------
sourcepath=/home/$user/cluster/cluster-install
copypath=/home/$user
targetpath=$copypath/cluster-install
softwarepath=/home/$user/cluster/software

# ----------------------------------------
# shiny
# ----------------------------------------
shinypath=$aapath/shinyapp

# ----------------------------------------
# java
# ----------------------------------------
java_home=$aapath/java
java_folder=jdk1.7.0_72
java_jar=jdk-7u72-linux-x64.gz

# ----------------------------------------
# scala
# ----------------------------------------
scala_home=$aapath/scala
scala_folder=scala-2.10.4
scala_jar=scala-2.10.4.tgz

# ----------------------------------------
# hadoop
# ----------------------------------------
hadoop_servers=$servers
hadoop_master=
hadoop_home=$aapath/hadoop
hadoop_folder=hadoop-2.7.1
hadoop_jar=$hadoop_folder.tar.gz

hadoop_hdfs_port=9000
hadoop_data_dir=$aapath1/hadoop/hdfs
hadoop_tmp_dir=$aapath1/hadoop/tmp
hadoop_log_dir=$aapath1/hadoop/logs
dfs_replication=2

# ----------------------------------------
# spark
# ----------------------------------------
spark_servers=$servers
spark_master=
spark_home=$aapath/spark
spark_folder=spark-1.5.2-bin-hadoop2.6
spark_jar=$spark_folder.tgz

spark_eventLog_dir=/var/log/spark
spark_tmp_dir=$spark_tmp_dir
spark_log_dir=$aapath1/spark/logs

# ----------------------------------------
# flume
# ----------------------------------------
flume_servers=
flume_home=$aapath/flume
flume_folder=apache-flume-1.6.0-bin
flume_jar=$flume_folder.tar.gz

# ----------------------------------------
# Zeppelin
# ----------------------------------------
zeppelin_notebook_dir=/home/$user/zeppelin_notebook

# ----------------------------------------
# Zookeeper
# ----------------------------------------
zookeeper_servers=$servers
zookeeper_port=2181
zookeeper_home=$aapath/zookeeper
zookeeper_folder=zookeeper-3.4.6
zookeeper_jar=$zookeeper_folder.tar.gz

declare -a zookeeper_servers_cwp=\(\"${zookeeper_servers//,/\" \"}\"\)
i=1
for server in "${zookeeper_servers_cwp[@]}"
do
if [ $i -eq 1 ] 
then
  zookeeper_connect=${server}:${zookeeper_port}  
else
  zookeeper_connect=${zookeeper_connect},${server}:${zookeeper_port}   
fi  
let i++                   
done

zookeeper_connect=$zookeeper_connect

# ----------------------------------------
# kafka
# ----------------------------------------
kafka_servers=$servers
kafka_home=$aapath/kafka
kafka_folder=kafka_2.10-0.9.0.1
kafka_jar=$kafka_folder.tgz
kafka_log_dir=$aapath1/kafka/logs

# ----------------------------------------

